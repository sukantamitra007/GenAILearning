{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjZtPU6kYPydXqvsT4a518",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukantamitra007/GenAILearning/blob/TransformerModel/GenA1_Doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "RftCf0HLFFxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GenAi Data Set **-\n",
        "1.  arXiv corpus\n",
        "\n",
        "you should always try to leverage a pretrained model — one as close as possible to the task you have at hand —\n",
        " and fine-tune it.\n",
        "\n",
        "**Transformer models :Broadly, can be grouped into three categories:**\n",
        "\n",
        "* PT-like (also called auto-regressive Transformer models)\n",
        "* BERT-like (also called auto-encoding Transformer models)\n",
        "* BART/T5-like (also called sequence-to-sequence Transformer models)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GFRN9z29_Nnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Each of these parts can be used independently, depending on the task:**\n",
        "\n",
        "  * Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
        "  * Decoder-only models: Good for generative tasks such as text generation.\n",
        "  * Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization.\n",
        "We will dive into those architectures independently in later sections."
      ],
      "metadata": {
        "id": "1mWvAt4AB5Iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**“Attention Is All You Need”!** -- Attention layer"
      ],
      "metadata": {
        "id": "60puRxFUC2h5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder models** : are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.\n",
        "\n",
        "Representatives of this family of models include:\n",
        "\n",
        "* ALBERT\n",
        "* BERT (rare Transformer models not built by scraping data from all over the internet)\n",
        "* DistilBERT\n",
        "* ELECTRA\n",
        "* RoBERTa\n",
        "\n",
        "**Decoding Models** : These models are best suited for tasks involving text generation.\n",
        "\n",
        "Representatives of this family of models include:\n",
        "* CTRL\n",
        "* GPT\n",
        "* GPT-2\n",
        "* Transformer XL"
      ],
      "metadata": {
        "id": "lBDJIdNFEywi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder-decoder models** : Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering.\n",
        "\n",
        "Representatives of this family of models include:\n",
        "\n",
        "* BART\n",
        "* mBART\n",
        "* Marian\n",
        "* T5"
      ],
      "metadata": {
        "id": "C00b1JH6GEcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MM_UaNWy_M2k"
      }
    }
  ]
}