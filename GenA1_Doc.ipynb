{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQAv5iK3eF++5KtUxGBsR9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukantamitra007/GenAILearning/blob/TransformerModel/GenA1_Doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "RftCf0HLFFxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GenAi Data Set **-\n",
        "1.  arXiv corpus\n",
        "\n",
        "you should always try to leverage a pretrained model ‚Äî one as close as possible to the task you have at hand ‚Äî\n",
        " and fine-tune it.\n",
        "\n",
        "**Transformer models :Broadly, can be grouped into three categories:**\n",
        "\n",
        "* PT-like (also called auto-regressive Transformer models)\n",
        "* BERT-like (also called auto-encoding Transformer models)\n",
        "* BART/T5-like (also called sequence-to-sequence Transformer models)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GFRN9z29_Nnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Each of these parts can be used independently, depending on the task:**\n",
        "\n",
        "  * Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
        "  * Decoder-only models: Good for generative tasks such as text generation.\n",
        "  * Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization.\n",
        "We will dive into those architectures independently in later sections."
      ],
      "metadata": {
        "id": "1mWvAt4AB5Iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚ÄúAttention Is All You Need‚Äù!** -- Attention layer"
      ],
      "metadata": {
        "id": "60puRxFUC2h5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder models** : are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.\n",
        "\n",
        "Representatives of this family of models include:\n",
        "\n",
        "* ALBERT\n",
        "* BERT (rare Transformer models not built by scraping data from all over the internet)\n",
        "* DistilBERT\n",
        "* ELECTRA\n",
        "* RoBERTa\n",
        "\n",
        "**Decoding Models** : These models are best suited for tasks involving text generation.\n",
        "\n",
        "Representatives of this family of models include:\n",
        "* CTRL\n",
        "* GPT\n",
        "* GPT-2\n",
        "* Transformer XL"
      ],
      "metadata": {
        "id": "lBDJIdNFEywi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder-decoder models** : Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering.\n",
        "\n",
        "Representatives of this family of models include:\n",
        "\n",
        "* BART\n",
        "* mBART\n",
        "* Marian\n",
        "* T5"
      ],
      "metadata": {
        "id": "C00b1JH6GEcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Behind the Pipeline"
      ],
      "metadata": {
        "id": "MM_UaNWy_M2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece] #comes with all the required dependencies"
      ],
      "metadata": {
        "id": "-XZuMk59tMdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "-yNsUT-vtFp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing with a tokenizer :**\n",
        "\n",
        "All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the Model Hub. To do this, we use the AutoTokenizer class and its from_pretrained() method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model‚Äôs tokenizer and cache it (so it‚Äôs only downloaded the first time you run the code below)."
      ],
      "metadata": {
        "id": "re3-WTWUti2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint) # THis will retrun the tokenizer with which actual model has been toenize.\n",
        "\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhzqPoottlAp",
        "outputId": "290ac99d-ea81-4cbf-a18c-1962058d3ff7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102],\n",
            "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Going through the model :**\n",
        "We can download our pretrained model the same way we did with our tokenizer. ü§ó Transformers provides an AutoModel class which also has a from_pretrained() method:"
      ],
      "metadata": {
        "id": "LQcg4KGlu7Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "wJW0C1iru5jm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A high-dimensional vector?\n",
        "\n",
        "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
        "\n",
        "Batch size: The number of sequences processed at a time (2 in our example).\n",
        "Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
        "Hidden size: The vector dimension of each model input.\n",
        "It is said to be ‚Äúhigh dimensional‚Äù because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more).\n",
        "\n",
        "We can see this if we feed the inputs we preprocessed to our model:"
      ],
      "metadata": {
        "id": "MofiIYVY0aNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtjSl25Z0J5v",
        "outputId": "d2a795e2-5863-4e57-cfb3-a14a77cccc8d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 16, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For sequence classification use below class"
      ],
      "metadata": {
        "id": "Peg3s7c81y0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "hMoc7PUB1uA3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Postprocessing the output**"
      ],
      "metadata": {
        "id": "-2FcST_X3P4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits.shape)\n",
        "print(outputs.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXanBq_716Wo",
        "outputId": "6370460a-f04b-4061-894b-412bf558f809"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n",
            "tensor([[-1.5607,  1.6123],\n",
            "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy"
      ],
      "metadata": {
        "id": "6o-WXwN_2cc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5OPbt6f2btm",
        "outputId": "0ed87f2c-89d9-4f16-e029-63578b6e108e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4.0195e-02, 9.5980e-01],\n",
            "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading a Transformer model that is already trained is simple ‚Äî we can do this using the from_pretrained() method:"
      ],
      "metadata": {
        "id": "b-_gtnBp-798"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "el1NbAQD_BkW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving methods**\n",
        "\n",
        "Saving a model is as easy as loading one ‚Äî we use the save_pretrained() method, which is analogous to the from_pretrained() method:"
      ],
      "metadata": {
        "id": "6oPf-qv7_Ip8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"C:\\\\Education\\\\GenAI\\\\bert-base-cased_model\")"
      ],
      "metadata": {
        "id": "HDWVprhv_PoZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a Transformer model for inference"
      ],
      "metadata": {
        "id": "QPQcb0VXBFqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequences = [\n",
        "    [101, 7592, 999, 102],\n",
        "    [101, 4658, 1012, 102],\n",
        "    [101, 3835, 999, 102],\n",
        "]"
      ],
      "metadata": {
        "id": "HxN2hMBGAy9S"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model_inputs = torch.tensor(encoded_sequences)\n",
        "print(model_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOpIzgCrA4ZY",
        "outputId": "174c4a37-d504-429a-ad40-98450cce65e4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 101, 7592,  999,  102],\n",
            "        [ 101, 4658, 1012,  102],\n",
            "        [ 101, 3835,  999,  102]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(model_inputs)"
      ],
      "metadata": {
        "id": "xB8O-rcUA-zv"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "4IBYX8dQEX4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "tokens = tokenizer.tokenize(\"Using a Transformer network is simple\") # Create token\n"
      ],
      "metadata": {
        "id": "TThAJW9xEjfd"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens) #Conver token into ids\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zde-ZjLkKONm",
        "outputId": "212e67cf-c0ea-4e24-e5a6-9a9b68ed212f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n",
            "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Using a Transformer network is simple\") #first two steps are combined here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c9NZn6KKmGO",
        "outputId": "cf9aa5ac-4be5-4934-87b0-0c08504bd983"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014]) # Decoding\n",
        "print(decoded_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnlX7gAXLDiX",
        "outputId": "35f8f0f1-c978-4495-d672-0b63c3bfdd48"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using a transformer network is simple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQqeB3xdN2_m",
        "outputId": "89b54c43-43f2-4e39-dfe1-0d70bd9b3812"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Tokenization Process**"
      ],
      "metadata": {
        "id": "4Qk3YYKFQpOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "output = model(**tokens)"
      ],
      "metadata": {
        "id": "5andJA1LQo0j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}